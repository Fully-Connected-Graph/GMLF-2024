{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from utils import save_processed_data, make_cyclic_features, plot_temperature_over_time, force_save_data\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from scipy.stats import entropy\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import imblearn\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "import os\n",
    "import smogn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Processing\n",
    "\n",
    "Here we just remove id and measurement_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_process_and_save(filepath):\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # drop id column\n",
    "    df = df.drop(columns=['measurement_time'])\n",
    "    save_processed_data(df, filepath, 'minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/minimal/train.csv\n",
      "Saving the file in data/minimal/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "minimal_process_and_save('data/raw/train.csv')\n",
    "minimal_process_and_save('data/raw/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Processing\n",
    "\n",
    "The first feature enrichment step. Extract Year, Month, Day and Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time(filepath):\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # measurement_time is of the form 'YYYY-MM-DD HH:MM:SS'\n",
    "    # Minutes and seconds are always 00\n",
    "    # Extract the year, month, day, hour\n",
    "    \n",
    "    df['measurement_time'] = pd.to_datetime(df['measurement_time'])\n",
    "    \n",
    "    df['year'] = df['measurement_time'].dt.year\n",
    "    df['month'] = df['measurement_time'].dt.month\n",
    "    df['day'] = df['measurement_time'].dt.day\n",
    "    df['hour'] = df['measurement_time'].dt.hour\n",
    "    \n",
    "    df = df.drop(columns=['measurement_time'])\n",
    "    \n",
    "    # Make them categorical\n",
    "    df['year'] = df['year'].astype('category')\n",
    "    df['month'] = df['month'].astype('category')\n",
    "    df['day'] = df['day'].astype('category')\n",
    "    df['hour'] = df['hour'].astype('category')\n",
    "    \n",
    "    save_processed_data(df, filepath, 'basic_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/basic_time/train.csv\n",
      "Saving the file in data/basic_time/test.csv\n"
     ]
    }
   ],
   "source": [
    "extract_time('data/day_of_week/train.csv')\n",
    "extract_time('data/day_of_week/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclic Processing\n",
    "\n",
    "Get sin and cos of desired features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cyclic(filepath):\n",
    "    # Assume we continue from basic_time\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # Make cyclic features for month, day, hour, wind_direction\n",
    "    df = make_cyclic_features(df, 'month', 12)\n",
    "    df = make_cyclic_features(df, 'day', 31) # 31 days in the longest month\n",
    "    df = make_cyclic_features(df, 'hour', 24)\n",
    "    df = make_cyclic_features(df, 'wind_direction', 360)\n",
    "    \n",
    "    # Drop year\n",
    "    df = df.drop(columns=['year'])\n",
    "    \n",
    "    save_processed_data(df, filepath, 'cyclic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/cyclic/train.pkl\n",
      "Saving the file in data/cyclic/test.pkl\n"
     ]
    }
   ],
   "source": [
    "make_cyclic('data/no_year/train.pkl')\n",
    "make_cyclic('data/no_year/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for distribution change via removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_removal(filepath, type='year'):\n",
    "    \n",
    "    # Continues from basic_time\n",
    "    # If type is year, remove all data points from 2023\n",
    "    # If type is month, only keep data points between 08 and 12\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # Print number of data points originally\n",
    "    print(f'Original number of data points: {len(df)}')\n",
    "    \n",
    "    # Do not change test data\n",
    "    if 'test' not in filepath:\n",
    "        if type == 'year':\n",
    "            df = df[df['year'] != 2023]\n",
    "        elif type == 'month':\n",
    "            # Convert month to int\n",
    "            df['month'] = df['month'].astype(int)\n",
    "            df = df[(df['month'] >= 8) & (df['month'] <= 11)]\n",
    "            # Convert back to category\n",
    "            df['month'] = df['month'].astype('category')\n",
    "        elif type == 'simple_year':\n",
    "            # Drop all 2023 data points, then all time features\n",
    "            df = df[df['year'] != 2023]\n",
    "            df = df.drop(columns=['month', 'day'])   \n",
    "    \n",
    "    if 'test' in filepath:   \n",
    "        if type == 'simple_year':\n",
    "            df = df.drop(columns=['month', 'day'])            \n",
    "    \n",
    "    # Print number of data points\n",
    "    print(f'Number of data points: {len(df)}')\n",
    "    \n",
    "    save_processed_data(df, filepath, f'no_{type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of data points: 7047\n",
      "Number of data points: 5583\n",
      "Saving the file in data/no_simple_year/train.pkl\n",
      "Original number of data points: 1762\n",
      "Number of data points: 1762\n",
      "Saving the file in data/no_simple_year/test.pkl\n"
     ]
    }
   ],
   "source": [
    "time_based_removal('data/basic_time/train.pkl', 'simple_year')\n",
    "time_based_removal('data/basic_time/test.pkl', 'simple_year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just make wind and hour cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wind_direction_cyclic(filepath):\n",
    "    \n",
    "    # Continues from no_year\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # Make wind direction cyclic\n",
    "    df = make_cyclic_features(df, 'wind_direction', 360)\n",
    "    \n",
    "    # Make hour cyclic\n",
    "    df = make_cyclic_features(df, 'hour', 24)\n",
    "    \n",
    "    # Make day of week cyclic, 0 is Monday\n",
    "    df = make_cyclic_features(df, 'day_of_week', 7)\n",
    "    \n",
    "    save_processed_data(df, filepath, 'dow_cyclic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/dow_cyclic/train.pkl\n",
      "Saving the file in data/dow_cyclic/test.pkl\n"
     ]
    }
   ],
   "source": [
    "make_wind_direction_cyclic('data/no_simple_year/train.pkl')\n",
    "make_wind_direction_cyclic('data/no_simple_year/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_of_week(filepath):\n",
    "    # Used on raw data\n",
    "    \n",
    "    # if extension is not pkl, load the data\n",
    "    if filepath[-3:] != 'pkl':\n",
    "        df = pd.read_csv(filepath)\n",
    "    else:\n",
    "        df = pd.read_pickle(filepath)\n",
    "        \n",
    "    # Convert measurement_time to datetime\n",
    "    df['measurement_time'] = pd.to_datetime(df['measurement_time'])\n",
    "    \n",
    "    df['day_of_week'] = df['measurement_time'].dt.dayofweek\n",
    "    df['day_of_week'] = df['day_of_week'].astype('category')\n",
    "    \n",
    "    save_processed_data(df, filepath, 'day_of_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/day_of_week/train.csv\n",
      "Saving the file in data/day_of_week/test.csv\n"
     ]
    }
   ],
   "source": [
    "get_day_of_week('data/raw/train.csv')\n",
    "get_day_of_week('data/raw/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holiday_simplification(filepath):\n",
    "    # Simplify holidays to 1 and non-holidays to 0\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # Set 5 and 6 to 1, rest to 0\n",
    "    # Create new field\n",
    "    df['holiday'] = 0\n",
    "    df.loc[df['day_of_week'] == 5, 'holiday'] = 1\n",
    "    df.loc[df['day_of_week'] == 6, 'holiday'] = 1\n",
    "    \n",
    "    # Drop day_of_week\n",
    "    df = df.drop(columns=['day_of_week'])\n",
    "    \n",
    "    # Set to category\n",
    "    df['holiday'] = df['holiday'].astype('category')\n",
    "    \n",
    "    # Print ratio of holidays\n",
    "    print(f'Ratio of holidays: {len(df[df[\"holiday\"] == 1]) / len(df)}')\n",
    "    \n",
    "    save_processed_data(df, filepath, 'holiday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of holidays: 0.28371843095110155\n",
      "Saving the file in data/holiday/train.pkl\n",
      "Ratio of holidays: 0.27298524404086266\n",
      "Saving the file in data/holiday/test.pkl\n"
     ]
    }
   ],
   "source": [
    "# from no_simple_year\n",
    "holiday_simplification('data/no_simple_year/train.pkl')\n",
    "holiday_simplification('data/no_simple_year/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2\n",
    "\n",
    "Holiday seems to get the best results. We will stick with it\n",
    "\n",
    "## Dumb simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_data(filepath):\n",
    "    # Simplify data to only include the most important features\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "\n",
    "    # Drop sun_radiation_east, sun_radiation_west, sun_radiation_south, sun_radiation_north and sun_radiation_perpendicular\n",
    "    df = df.drop(columns=['sun_radiation_east', 'sun_radiation_west', 'sun_radiation_south', 'sun_radiation_north', 'sun_radiation_perpendicular', 'clouds', 'wind_direction', 'wind_speed'])\n",
    "    \n",
    "    save_processed_data(df, filepath, 'simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/simple/train.pkl\n",
      "Saving the file in data/simple/test.pkl\n"
     ]
    }
   ],
   "source": [
    "simplify_data('data/holiday/train.pkl')\n",
    "simplify_data('data/holiday/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making work hours explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def work_hours(filepath):\n",
    "    # Make hours cyclic\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # Convert hour to int\n",
    "    df['hour'] = df['hour'].astype(int)\n",
    "    \n",
    "    # Add a variable for work hours\n",
    "    df['work_hours'] = 0\n",
    "    df.loc[(df['hour'] >= 9) & (df['hour'] <= 17), 'work_hours'] = 1\n",
    "    \n",
    "    df = make_cyclic_features(df, 'hour', 24)\n",
    "    \n",
    "    save_processed_data(df, filepath, 'work_hours_cyclic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/work_hours_cyclic/train.pkl\n",
      "Saving the file in data/work_hours_cyclic/test.pkl\n"
     ]
    }
   ],
   "source": [
    "work_hours('data/simple/train.pkl')\n",
    "work_hours('data/simple/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readd the day_of_week feature, does not seem to be useful\n",
    "def add_day_of_week(filepath):\n",
    "    # Used on raw data\n",
    "    \n",
    "    # After last / but replace .csv with .pkl\n",
    "    file_name = filepath.split('/')[-1].replace('.pkl', '.csv')\n",
    "    \n",
    "    # I just realized that this is a constant\n",
    "    df = pd.read_csv(f'data/raw/{file_name}')\n",
    "\n",
    "        \n",
    "    # Convert measurement_time to datetime\n",
    "    df['measurement_time'] = pd.to_datetime(df['measurement_time'])\n",
    "    \n",
    "    df['day_of_week'] = df['measurement_time'].dt.dayofweek\n",
    "    df['day_of_week'] = df['day_of_week'].astype(float)\n",
    "    \n",
    "    # Add the day_of_week feature to new_df\n",
    "    new_df = pd.read_pickle(filepath)\n",
    "    new_df['day_of_week'] = df['day_of_week']\n",
    "    \n",
    "    # Make it cyclic\n",
    "    new_df = make_cyclic_features(new_df, 'day_of_week', 7)\n",
    "    \n",
    "    \n",
    "    save_processed_data(new_df, filepath, 'returned_day_of_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/returned_day_of_week/train.pkl\n",
      "Saving the file in data/returned_day_of_week/test.pkl\n"
     ]
    }
   ],
   "source": [
    "add_day_of_week('data/work_hours_cyclic/train.pkl')\n",
    "add_day_of_week('data/work_hours_cyclic/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(folder_path, type='standard'):\n",
    "    \n",
    "    train_path = f'{folder_path}/train.pkl'\n",
    "    test_path = f'{folder_path}/test.pkl'\n",
    "    \n",
    "    df_train = pd.read_pickle(train_path)\n",
    "    df_test = pd.read_pickle(test_path)\n",
    "    \n",
    "    float_columns = df_train.select_dtypes(include=['float64']).columns\n",
    "    \n",
    "    # remove target\n",
    "    float_columns = float_columns.drop('target')\n",
    "    \n",
    "    if type == 'standard':\n",
    "        scaler = sk.preprocessing.StandardScaler()\n",
    "    elif type == 'minmax':\n",
    "        scaler = sk.preprocessing.MinMaxScaler()\n",
    "        # power transforms\n",
    "    elif type == 'power':\n",
    "        scaler = sk.preprocessing.PowerTransformer()\n",
    "    elif type == 'robust':\n",
    "        scaler = sk.preprocessing.RobustScaler()\n",
    "    else:\n",
    "        raise ValueError('Invalid type')\n",
    "    \n",
    "    # Fit the scaler on the training data\n",
    "    scaler.fit(df_train[float_columns])\n",
    "    \n",
    "    scaled_train = scaler.transform(df_train[float_columns])\n",
    "    scaled_test = scaler.transform(df_test[float_columns])\n",
    "    \n",
    "    df_train[float_columns] = scaled_train\n",
    "    df_test[float_columns] = scaled_test\n",
    "    \n",
    "    \n",
    "    save_processed_data(df_train, train_path, f'{type}_scaler')\n",
    "    save_processed_data(df_test, test_path, f'{type}_scaler')\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/standard_scaler/train.pkl\n",
      "Saving the file in data/standard_scaler/test.pkl\n"
     ]
    }
   ],
   "source": [
    "scaled_train_df, scaled_test_df = standardize('data/standard', 'standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making categories from sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_discrete_sensor(filepath):\n",
    "    # Too much info lost\n",
    "    \n",
    "    # Add a discrete sensor feature\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # For source_1_temperature, 0 to 24, 24 to 38, 38 to all left\n",
    "    df['discrete_sensor_1'] = 0\n",
    "    df.loc[df['source_1_temperature'] > 24, 'discrete_sensor_1'] = 1\n",
    "    df.loc[df['source_1_temperature'] > 38, 'discrete_sensor_1'] = 2\n",
    "    \n",
    "    df['discrete_sensor_1'] = df['discrete_sensor_1'].astype('category')\n",
    "    \n",
    "    # For source_2_temperature, 0 to 17, 17 to 20, 20 to all left\n",
    "    df['discrete_sensor_2'] = 0\n",
    "    df.loc[df['source_2_temperature'] > 17, 'discrete_sensor_2'] = 1\n",
    "    df.loc[df['source_2_temperature'] > 20, 'discrete_sensor_2'] = 2\n",
    "    \n",
    "    df['discrete_sensor_2'] = df['discrete_sensor_2'].astype('category')\n",
    "    \n",
    "    # For source_3_temperature, 0 to 17, 17 to 19, 19 to 21, 21 to all left\n",
    "    df['discrete_sensor_3'] = 0\n",
    "    df.loc[df['source_3_temperature'] > 17, 'discrete_sensor_3'] = 1\n",
    "    df.loc[df['source_3_temperature'] > 19, 'discrete_sensor_3'] = 2\n",
    "    df.loc[df['source_3_temperature'] > 21, 'discrete_sensor_3'] = 3\n",
    "    \n",
    "    df['discrete_sensor_3'] = df['discrete_sensor_3'].astype('category')\n",
    "    \n",
    "    # For source_4_temperature, 0 to 19, 19 to 20, 20 to 23, 23 to 28, 28 to all left\n",
    "    df['discrete_sensor_4'] = 0\n",
    "    df.loc[df['source_4_temperature'] > 19, 'discrete_sensor_4'] = 1\n",
    "    df.loc[df['source_4_temperature'] > 20, 'discrete_sensor_4'] = 2\n",
    "    df.loc[df['source_4_temperature'] > 23, 'discrete_sensor_4'] = 3\n",
    "    df.loc[df['source_4_temperature'] > 28, 'discrete_sensor_4'] = 4\n",
    "    \n",
    "    # Drop the original temperature columns\n",
    "    df = df.drop(columns=['source_1_temperature', 'source_2_temperature', 'source_3_temperature', 'source_4_temperature'])\n",
    "    \n",
    "    save_processed_data(df, filepath, 'discrete_sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/discrete_sensor/train.pkl\n",
      "Saving the file in data/discrete_sensor/test.pkl\n"
     ]
    }
   ],
   "source": [
    "add_discrete_sensor('data/work_hours_cyclic/train.pkl')\n",
    "add_discrete_sensor('data/work_hours_cyclic/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_difference_and_mean(filepath):\n",
    "    # difference between mean_room_temperature and outside_temperature\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    df['temp_diff'] = df['mean_room_temperature'] - df['outside_temperature']\n",
    "    \n",
    "    # Drop the original columns\n",
    "    df = df.drop(columns=['mean_room_temperature', 'outside_temperature'])\n",
    "    \n",
    "    # Create a column for mean of sensor 1 and 4 and then 2 and 3\n",
    "    df['heating_temp'] = (df['source_1_temperature'] + df['source_4_temperature']) / 2\n",
    "    df['ventilation_temp'] = (df['source_2_temperature'] + df['source_3_temperature']) / 2\n",
    "    \n",
    "    # Drop the original columns\n",
    "    # df = df.drop(columns=['source_1_temperature', 'source_2_temperature', 'source_3_temperature', 'source_4_temperature'])\n",
    "    \n",
    "    \n",
    "    save_processed_data(df, filepath, 'temp_and_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/temp_and_mean/train.pkl\n",
      "Saving the file in data/temp_and_mean/test.pkl\n"
     ]
    }
   ],
   "source": [
    "temp_difference_and_mean('data/work_hours_cyclic/train.pkl')\n",
    "temp_difference_and_mean('data/work_hours_cyclic/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readd month, get month from basic_time\n",
    "def add_month(filepath):\n",
    "    # Used on raw data\n",
    "    \n",
    "    df = pd.read_csv('data/basic_time/train.csv')\n",
    "\n",
    "    \n",
    "    # Add the month feature to new_df\n",
    "    new_df = pd.read_pickle(filepath)\n",
    "    new_df['month'] = df['month']\n",
    "    \n",
    "    # Convert month into Dutch seasons. 1 value for each season, use loops \n",
    "    new_df['season'] = 0\n",
    "    for i in range(1, 13):\n",
    "        if i in [3, 4, 5]:\n",
    "            new_df.loc[new_df['month'] == i, 'season'] = 1\n",
    "        elif i in [6, 7, 8]:\n",
    "            new_df.loc[new_df['month'] == i, 'season'] = 2\n",
    "        elif i in [9, 10, 11]:\n",
    "            new_df.loc[new_df['month'] == i, 'season'] = 3\n",
    "        else:\n",
    "            new_df.loc[new_df['month'] == i, 'season'] = 4\n",
    "            \n",
    "    new_df['season'] = new_df['season'].astype('category')\n",
    "    \n",
    "    # month cyclic\n",
    "    new_df = make_cyclic_features(new_df, 'month', 12)\n",
    "    \n",
    "    save_processed_data(new_df, filepath, 'returned_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/returned_month/train.pkl\n",
      "Saving the file in data/returned_month/test.pkl\n"
     ]
    }
   ],
   "source": [
    "add_month('data/work_hours_cyclic/train.pkl')\n",
    "add_month('data/work_hours_cyclic/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special days\n",
    "Does not seem to help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_holidays(filepath):\n",
    "    \n",
    "    # Account for special days like Christmas, Easter, etc.\n",
    "    # Month: Day\n",
    "    special_days = {\n",
    "        1: [1], # New Year's Day, 1 January\n",
    "        3: [29, 31], # Good Friday, 29 March and Easter Monday, 31 March\n",
    "        4: [1, 27], # Easter Sunday, 1 April and King's Day, 27 April\n",
    "        5: [5, 9, 19, 20], # Liberation Day, 5 May and Ascension Day, 9 May and Whit Sunday, 19 May and Whit Monday, 20 May\n",
    "        12: [25, 26], # Christmas and boxing day, 25 and 26 December\n",
    "    }\n",
    "    \n",
    "    # Load basic_time\n",
    "    original_time = pd.read_csv('data/basic_time/train.csv')\n",
    "    \n",
    "    # Set holiday to 1 where it is a special day based on time mentioned in original_time\n",
    "    current_df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # Use holiday variable to store special days, it already exists\n",
    "    for month, days in special_days.items():\n",
    "        for day in days:\n",
    "            current_df.loc[(original_time['month'] == month) & (original_time['day'] == day), 'holiday'] = 1\n",
    "            \n",
    "    save_processed_data(current_df, filepath, 'special_holidays')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/special_holidays/train.pkl\n",
      "Saving the file in data/special_holidays/test.pkl\n"
     ]
    }
   ],
   "source": [
    "add_special_holidays('data/work_hours_cyclic/train.pkl')\n",
    "add_special_holidays('data/work_hours_cyclic/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readd_radiations(filepath):\n",
    "    \n",
    "    # After last / but replace .csv with .pkl\n",
    "    file_name = filepath.split('/')[-1].replace('.pkl', '.csv')\n",
    "    \n",
    "    original_df = pd.read_csv(f'data/raw/{file_name}')\n",
    "    \n",
    "    # Load raw\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # Add the radiations back\n",
    "    df['sun_radiation_east'] = original_df['sun_radiation_east']\n",
    "    df['sun_radiation_west'] = original_df['sun_radiation_west']\n",
    "    df['sun_radiation_south'] = original_df['sun_radiation_south']\n",
    "    df['sun_radiation_north'] = original_df['sun_radiation_north']\n",
    "    df['sun_radiation_perpendicular'] = original_df['sun_radiation_perpendicular']\n",
    "    \n",
    "    save_processed_data(df, filepath, 'readded_radiations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/readded_radiations/train.pkl\n",
      "Saving the file in data/readded_radiations/test.pkl\n"
     ]
    }
   ],
   "source": [
    "readd_radiations('data/work_hours_cyclic/train.pkl')\n",
    "readd_radiations('data/work_hours_cyclic/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/simple_hvac/train.pkl\n",
      "Saving the file in data/simple_hvac/test.pkl\n"
     ]
    }
   ],
   "source": [
    "def drop_sources(filepath):\n",
    "    # Drop source columns\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    df = df.drop(columns=['source_4_temperature'])\n",
    "    \n",
    "    save_processed_data(df, filepath, 'simple_hvac')\n",
    "\n",
    "drop_sources('data/work_hours_cyclic/train.pkl')\n",
    "drop_sources('data/work_hours_cyclic/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/rounded/train.pkl\n",
      "Saving the file in data/rounded/test.pkl\n"
     ]
    }
   ],
   "source": [
    "def rounding(filepath):\n",
    "    # Round all floats except target to X decimals\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    float_columns = df.select_dtypes(include=['float64']).columns\n",
    "    \n",
    "    # remove target\n",
    "    filename = filepath.split('/')[-1]\n",
    "    \n",
    "    if 'train' in filename:\n",
    "        float_columns = float_columns.drop('target')\n",
    "    \n",
    "    df[float_columns] = df[float_columns].round(4)\n",
    "    \n",
    "    # Drop year column\n",
    "    df = df.drop(columns=['year'])\n",
    "    \n",
    "    save_processed_data(df, filepath, 'rounded')\n",
    "    \n",
    "rounding('data/work_hours_cyclic/train.pkl')\n",
    "rounding('data/work_hours_cyclic/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smogn(filepath):\n",
    "    \n",
    "    # Apply smogn to the data\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Apply smogn\n",
    "    df = smogn.smoter(data = df, y = 'target')\n",
    "    \n",
    "    middle_path = '/'.join(filepath.split('/')[1:-1])\n",
    "    folder_path = '/'.join(filepath.split('/')[:-1])\n",
    "    save_processed_data(df, f'{folder_path}/smogn.pkl', middle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_matrix: 100%|##########| 878/878 [01:46<00:00,  8.27it/s]\n",
      "synth_matrix: 100%|##########| 878/878 [00:01<00:00, 569.82it/s]\n",
      "r_index: 100%|##########| 157/157 [00:00<00:00, 1143.57it/s]\n",
      "c:\\Users\\User\\.conda\\envs\\energy\\Lib\\site-packages\\smogn\\smoter.py:284: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "5576    0\n",
      "5579    0\n",
      "5580    0\n",
      "5581    0\n",
      "5582    0\n",
      "Name: holiday, Length: 4704, dtype: category\n",
      "Categories (2, int64): [0, 1]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  data_new.iloc[:, j] = data_new.iloc[:, j].astype(feat_dtypes_orig[j])\n",
      "c:\\Users\\User\\.conda\\envs\\energy\\Lib\\site-packages\\smogn\\smoter.py:284: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0         0\n",
      "1       NaN\n",
      "2         1\n",
      "3         1\n",
      "4       NaN\n",
      "       ... \n",
      "5576      0\n",
      "5579      1\n",
      "5580      1\n",
      "5581      1\n",
      "5582      1\n",
      "Name: work_hours, Length: 4704, dtype: category\n",
      "Categories (2, int64): [0, 1]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data_new.iloc[:, j] = data_new.iloc[:, j].astype(feat_dtypes_orig[j])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/standard_scaler/smogn.pkl\n"
     ]
    }
   ],
   "source": [
    "apply_smogn('data/standard_scaler/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make work_hours and holiday categorical in smogn\n",
    "def make_categorical(filepath):\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    df['work_hours'] = df['work_hours'].astype('category')\n",
    "    df['holiday'] = df['holiday'].astype('category')\n",
    "    \n",
    "    save_processed_data(df, filepath, 'smogn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/smogn/train.pkl\n"
     ]
    }
   ],
   "source": [
    "make_categorical('data/smogn/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_sqrt(filepath):\n",
    "    # Take the square root of the source_x_temperature columns\n",
    "    # Remove the original columns\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        df[f'source_{i}_temperature'] = np.sqrt(df[f'source_{i}_temperature'])\n",
    "    \n",
    "    save_processed_data(df, filepath, 'sqrt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/sqrt/train.pkl\n",
      "Saving the file in data/sqrt/test.pkl\n"
     ]
    }
   ],
   "source": [
    "take_sqrt('data/standard/train.pkl')\n",
    "take_sqrt('data/standard/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readd_columns(filepath):\n",
    "    \n",
    "    # After last / but replace .csv with .pkl\n",
    "    file_name = filepath.split('/')[-1].replace('.pkl', '.csv')\n",
    "    \n",
    "    original_df = pd.read_csv(f'data/raw/{file_name}')\n",
    "    \n",
    "    # Load raw\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    columns_to_add = ['clouds', 'sun_radiation_west', 'sun_radiation_north', 'sun_radiation_perpendicular', 'sun_radiation_east']\n",
    "    \n",
    "    for column in columns_to_add:\n",
    "        df[column] = original_df[column]\n",
    "    \n",
    "    save_processed_data(df, filepath, 'retry_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/retry_data/train.pkl\n",
      "Saving the file in data/retry_data/test.pkl\n"
     ]
    }
   ],
   "source": [
    "readd_columns('data/standard/train.pkl')\n",
    "readd_columns('data/standard/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_target(filepath):\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    if 'target' in df.columns:\n",
    "        df['target'] = df['target'].round(4)\n",
    "    \n",
    "    save_processed_data(df, filepath, 'rounded_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/rounded_target/train.pkl\n",
      "Saving the file in data/rounded_target/test.pkl\n"
     ]
    }
   ],
   "source": [
    "round_target('data/standard/train.pkl')\n",
    "round_target('data/standard/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readd_datetime(filepath):\n",
    "    \n",
    "    # After last / but replace .csv with .pkl\n",
    "    file_name = filepath.split('/')[-1].replace('.pkl', '.csv')\n",
    "    \n",
    "    original_df = pd.read_csv(f'data/raw/{file_name}')\n",
    "    \n",
    "    # Load raw\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    df['measurement_time'] = original_df['measurement_time']\n",
    "    \n",
    "    save_processed_data(df, filepath, 'standard_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/standard_date/train.pkl\n",
      "Saving the file in data/standard_date/test.pkl\n"
     ]
    }
   ],
   "source": [
    "readd_datetime('data/standard/train.pkl')\n",
    "readd_datetime('data/standard/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_kalman_filter(filepath):\n",
    "    # Apply Kalman filter to source_x_temperature columns\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        column = f'source_{i}_temperature'\n",
    "        \n",
    "        # Initialize Kalman filter\n",
    "        kf = KalmanFilter(dim_x=2, dim_z=1)\n",
    "        kf.x = np.array([df[column].iloc[0], 0])  # initial state (location and velocity)\n",
    "        kf.F = np.array([[1, 1], [0, 1]])        # state transition matrix\n",
    "        kf.H = np.array([[1, 0]])                # measurement function\n",
    "        kf.P *= 1000.                            # covariance matrix\n",
    "        kf.R = 10                                 # measurement noise\n",
    "        kf.Q = np.array([[0.1, 0], [0, 0.1]])    # process noise\n",
    "        \n",
    "        filtered_values = []\n",
    "        for value in df[column]:\n",
    "            kf.predict()\n",
    "            kf.update(value)\n",
    "            filtered_values.append(kf.x[0])\n",
    "        \n",
    "        df[column] = filtered_values\n",
    "        \n",
    "    save_processed_data(df, filepath, 'kalman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/kalman/train.pkl\n",
      "Saving the file in data/kalman/test.pkl\n"
     ]
    }
   ],
   "source": [
    "apply_kalman_filter('data/source_1_fixed/train.pkl')\n",
    "apply_kalman_filter('data/source_1_fixed/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_time(filepath):\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    df = df.drop(columns=['measurement_time'])\n",
    "    \n",
    "    save_processed_data(df, filepath, 'no_measurement_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/no_measurement_time/train.pkl\n",
      "Saving the file in data/no_measurement_time/test.pkl\n"
     ]
    }
   ],
   "source": [
    "drop_time('data/kalman/train.pkl')\n",
    "drop_time('data/kalman/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_timeline(filepath, n):\n",
    "    \n",
    "    # For each source_x_temperature, include the previous n values and the next n values\n",
    "    # For boundary cases, use the same value\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    for i in range(1, 5):\n",
    "        column = f'source_{i}_temperature'\n",
    "        \n",
    "        for j in range(1, n + 1):\n",
    "            df[f'{column}_prev_{j}'] = df[column].shift(j)\n",
    "            df[f'{column}_next_{j}'] = df[column].shift(-j)\n",
    "            \n",
    "    # Fill Nan values with mean of the column, take care of non-numeric columns\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'float64':\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "        else:\n",
    "            df[column] = df[column].fillna(df[column].mode()[0])\n",
    "            \n",
    "    # Check number of NaN values\n",
    "    print(f'Number of NaN values: {df.isna().sum().sum()}')\n",
    "            \n",
    "    save_processed_data(df, filepath, f'timeline_{n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values: 0\n",
      "Saving the file in data/timeline_3/train.pkl\n",
      "Number of NaN values: 0\n",
      "Saving the file in data/timeline_3/test.pkl\n"
     ]
    }
   ],
   "source": [
    "add_timeline('data/standard/train.pkl', 3)\n",
    "add_timeline('data/standard/test.pkl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_between_months(filepath):\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    # measurement_time must be present and converted to datetime\n",
    "    df['measurement_time'] = pd.to_datetime(df['measurement_time'])\n",
    "    \n",
    "    # Replace source_1_temperature with mean of data set betweem months 7 and 9\n",
    "    mean_temp = df[(df['measurement_time'].dt.month >= 7) & (df['measurement_time'].dt.month <= 9)]['source_1_temperature'].mean()\n",
    "    \n",
    "    # replace selected timeframe with mean\n",
    "    df.loc[(df['measurement_time'].dt.month >= 7) & (df['measurement_time'].dt.month <= 9), 'source_1_temperature'] = mean_temp\n",
    "    \n",
    "    # Drop measurement_time\n",
    "    # df = df.drop(columns=['measurement_time'])\n",
    "    \n",
    "    # Add noise to affected source_1_temperature values\n",
    "    df.loc[(df['source_1_temperature'] == mean_temp), 'source_1_temperature'] += 10 * np.random.uniform(0.01, 0.999, len(df[df['source_1_temperature'] == mean_temp]))\n",
    "  \n",
    "    \n",
    "    save_processed_data(df, filepath, 'source_1_fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/source_1_fixed/train.pkl\n"
     ]
    }
   ],
   "source": [
    "drop_between_months('data/standard_date/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agricultural_seasons(filepath):\n",
    "    \n",
    "    # After last / but replace .csv with .pkl\n",
    "    file_name = filepath.split('/')[-1].replace('.pkl', '.csv')\n",
    "    \n",
    "    original_df = pd.read_csv(f'data/basic_time/{file_name}')\n",
    "    \n",
    "    # Drop 2023 data points\n",
    "    original_df = original_df[original_df['year'] != 2023]\n",
    "    \n",
    "    # Create a new column for agricultural seasons\n",
    "    # If between 4 to 10 -> 1, else 0\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    df['agricultural_season'] = 0\n",
    "    \n",
    "    df.loc[(original_df['month'] >= 4) & (original_df['month'] <= 10), 'agricultural_season'] = 1\n",
    "    \n",
    "    df['agricultural_season'] = df['agricultural_season'].astype('category')\n",
    "    \n",
    "    save_processed_data(df, filepath, 'agricultural_seasons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/agricultural_seasons/train.pkl\n",
      "Saving the file in data/agricultural_seasons/test.pkl\n"
     ]
    }
   ],
   "source": [
    "agricultural_seasons('data/degree_day_dynamic/train.pkl')\n",
    "agricultural_seasons('data/degree_day_dynamic/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_day_dynamic(filepath):\n",
    "    \n",
    "    T_h = 12\n",
    "    T_c = 15\n",
    "    \n",
    "    # T_max and T_min are the maximum and minimum outside_temperature of the day\n",
    "    # Cooling = max((T_max + T_min) / 2 - T_c, 0)\n",
    "    # Heating = max(T_h - (T_max + T_min) / 2, 0)\n",
    "    \n",
    "    file_name = filepath.split('/')[-1].replace('.pkl', '.csv')\n",
    "    \n",
    "    # Used to determine day\n",
    "    original_df = pd.read_csv(f'data/basic_time/{file_name}')\n",
    "    \n",
    "    # Drop year=2023\n",
    "    original_df = original_df[original_df['year'] != 2023]\n",
    "    \n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    df['degree_day_heating'] = 0.0\n",
    "    df['degree_day_cooling'] = 0.0\n",
    "    \n",
    "    original_df['date'] = pd.to_datetime(original_df[['year', 'month', 'day']])\n",
    "\n",
    "    for date in original_df['date'].unique():\n",
    "        day_df = original_df[original_df['date'] == date]\n",
    "        T_max = day_df['outside_temperature'].max()\n",
    "        T_min = day_df['outside_temperature'].min()\n",
    "        \n",
    "        heating = max(T_h - (T_max + T_min) / 2, 0)\n",
    "        cooling = max((T_max + T_min) / 2 - T_c, 0)\n",
    "        \n",
    "        indices = day_df.index\n",
    "        df.loc[indices, 'degree_day_heating'] = heating\n",
    "        df.loc[indices, 'degree_day_cooling'] = cooling\n",
    "        \n",
    "    # Drop outside_temperature\n",
    "    # df = df.drop(columns=['outside_temperature'])\n",
    "    \n",
    "    save_processed_data(df, filepath, 'degree_day_dynamic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/degree_day_dynamic/train.pkl\n",
      "Saving the file in data/degree_day_dynamic/test.pkl\n"
     ]
    }
   ],
   "source": [
    "degree_day_dynamic('data/standard/train.pkl')\n",
    "degree_day_dynamic('data/standard/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readd_month(filepath):\n",
    "    \n",
    "    # After last / but replace .csv with .pkl\n",
    "    file_name = filepath.split('/')[-1].replace('.pkl', '.csv')\n",
    "    \n",
    "    original_df = pd.read_csv(f'data/basic_time/{file_name}')\n",
    "    \n",
    "    # Drop 2023 data points\n",
    "    original_df = original_df[original_df['year'] != 2023]\n",
    "    \n",
    "    # Load raw\n",
    "    df = pd.read_pickle(filepath)\n",
    "    \n",
    "    df['month'] = original_df['month']\n",
    "    \n",
    "    # Make month cyclic\n",
    "    df = make_cyclic_features(df, 'month', 12)\n",
    "    \n",
    "    save_processed_data(df, filepath, 'readded_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the file in data/readded_month/train.pkl\n",
      "Saving the file in data/readded_month/test.pkl\n"
     ]
    }
   ],
   "source": [
    "readd_month('data/standard/train.pkl')\n",
    "readd_month('data/standard/test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_spline_features(train_df, test_df, column_name, n_knots=5, degree=3):\n",
    "    \"\"\"\n",
    "    Encode a column using spline features.\n",
    "\n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training DataFrame containing the column to encode.\n",
    "    test_df (pd.DataFrame): Test DataFrame containing the column to encode.\n",
    "    column_name (str): Name of the column to encode.\n",
    "    n_knots (int): Number of knots to use. Default is 5.\n",
    "    degree (int): Degree of the spline. Default is 3 (cubic spline).\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine train and test data for fitting the spline transformer\n",
    "    combined_df = pd.concat([train_df[[column_name]], test_df[[column_name]]])\n",
    "\n",
    "    # Initialize SplineTransformer\n",
    "    spline = SplineTransformer(n_knots=n_knots, degree=degree, include_bias=False)\n",
    "\n",
    "    # Fit on combined data\n",
    "    spline.fit(combined_df)\n",
    "\n",
    "    # Transform both training and test data\n",
    "    train_spline = spline.transform(train_df[[column_name]])\n",
    "    test_spline = spline.transform(test_df[[column_name]])\n",
    "\n",
    "    # Create DataFrames with spline features\n",
    "    spline_feature_names = [f\"{column_name}_spline_{i}\" for i in range(train_spline.shape[1])]\n",
    "    train_spline_df = pd.DataFrame(train_spline, columns=spline_feature_names, index=train_df.index)\n",
    "    test_spline_df = pd.DataFrame(test_spline, columns=spline_feature_names, index=test_df.index)\n",
    "\n",
    "    # Concatenate spline features with original DataFrames\n",
    "    train_df = pd.concat([train_df.drop(columns=[column_name]), train_spline_df], axis=1)\n",
    "    test_df = pd.concat([test_df.drop(columns=[column_name]), test_spline_df], axis=1)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def sum_radiations(df):\n",
    "    \n",
    "    # Sum the radiations\n",
    "    df['total_radiation'] = df['sun_radiation_east'] + df['sun_radiation_west'] + df['sun_radiation_south'] + df['sun_radiation_north'] + df['sun_radiation_perpendicular']\n",
    "    \n",
    "    # Drop the original columns\n",
    "    df = df.drop(columns=['sun_radiation_east', 'sun_radiation_west', 'sun_radiation_south', 'sun_radiation_north', 'sun_radiation_perpendicular'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def apply_nystrom(train_df, test_df, columns, kernel='rbf', n_components=100):\n",
    "    \"\"\"\n",
    "    Apply the Nystrom method to approximate a kernel matrix.\n",
    "\n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training DataFrame.\n",
    "    test_df (pd.DataFrame): Test DataFrame.\n",
    "    columns (list): List of columns to apply the Nystrom method on.\n",
    "    kernel (str): Kernel type to use. Default is 'rbf'.\n",
    "    n_components (int): Number of components for the Nystrom approximation. Default is 100.\n",
    "    \"\"\"\n",
    "    # Initialize Nystroem transformer\n",
    "    nystroem = Nystroem(kernel=kernel, n_components=n_components, random_state=42)\n",
    "\n",
    "    # Combine train and test data for fitting the Nystroem transformer\n",
    "    combined_df = pd.concat([train_df[columns], test_df[columns]])\n",
    "    \n",
    "    # Fit on combined data\n",
    "    nystroem.fit(combined_df[columns])\n",
    "    \n",
    "    # Transform train data\n",
    "    train_transformed = nystroem.transform(train_df[columns])\n",
    "\n",
    "    # Transform test data\n",
    "    test_transformed = nystroem.transform(test_df[columns])\n",
    "\n",
    "    # Create DataFrames with Nystroem features\n",
    "    nystroem_feature_names = [f\"nystroem_{i}\" for i in range(train_transformed.shape[1])]\n",
    "    train_nystroem_df = pd.DataFrame(train_transformed, columns=nystroem_feature_names, index=train_df.index)\n",
    "    test_nystroem_df = pd.DataFrame(test_transformed, columns=nystroem_feature_names, index=test_df.index)\n",
    "\n",
    "    # Concatenate Nystroem features with original DataFrames\n",
    "    train_df = pd.concat([train_df.drop(columns=columns), train_nystroem_df], axis=1)\n",
    "    test_df = pd.concat([test_df.drop(columns=columns), test_nystroem_df], axis=1)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def apply_kernel_pca(train_df, test_df, columns, n_components=5, kernel='rbf'):\n",
    "    \"\"\"\n",
    "    Apply Kernel PCA to reduce dimensionality of specified columns.\n",
    "\n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training DataFrame.\n",
    "    test_df (pd.DataFrame): Test DataFrame.\n",
    "    columns (list): List of columns to apply Kernel PCA on.\n",
    "    n_components (int): Number of components to keep. Default is 5.\n",
    "    kernel (str): Kernel type to use in Kernel PCA. Default is 'rbf'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame, pd.DataFrame: Transformed training and test DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine data for fitting\n",
    "    combined_data = pd.concat([train_df[columns], test_df[columns]], ignore_index=True)\n",
    "\n",
    "    # Initialize Kernel PCA\n",
    "    kpca = KernelPCA(n_components=n_components, kernel=kernel, random_state=42)\n",
    "\n",
    "    # Fit Kernel PCA\n",
    "    kpca.fit(combined_data)\n",
    "\n",
    "    # Transform the data\n",
    "    train_kpca = kpca.transform(train_df[columns])\n",
    "    test_kpca = kpca.transform(test_df[columns])\n",
    "\n",
    "    # Create DataFrames with KPCA components\n",
    "    kpca_columns = [f\"kpca_{i+1}\" for i in range(n_components)]\n",
    "    train_kpca_df = pd.DataFrame(train_kpca, columns=kpca_columns, index=train_df.index)\n",
    "    test_kpca_df = pd.DataFrame(test_kpca, columns=kpca_columns, index=test_df.index)\n",
    "\n",
    "    # Drop original columns and add KPCA components\n",
    "    train_df = pd.concat([train_df.drop(columns=columns), train_kpca_df], axis=1)\n",
    "    test_df = pd.concat([test_df.drop(columns=columns), test_kpca_df], axis=1)\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from the beginning\n",
    "\n",
    "# Load the data from raw which is csv\n",
    "train_raw = pd.read_csv('data/raw/train.csv')\n",
    "test_raw = pd.read_csv('data/raw/test.csv')\n",
    "\n",
    "# Convert measurement_time to datetime\n",
    "train_raw['measurement_time'] = pd.to_datetime(train_raw['measurement_time'])\n",
    "test_raw['measurement_time'] = pd.to_datetime(test_raw['measurement_time'])\n",
    "\n",
    "# Added is working hours categorical (9 to 17)\n",
    "train_raw['work_hours'] = 0\n",
    "train_raw.loc[(train_raw['measurement_time'].dt.hour >= 9) & (train_raw['measurement_time'].dt.hour <= 17), 'work_hours'] = 1\n",
    "\n",
    "test_raw['work_hours'] = 0\n",
    "test_raw.loc[(test_raw['measurement_time'].dt.hour >= 9) & (test_raw['measurement_time'].dt.hour <= 17), 'work_hours'] = 1\n",
    "\n",
    "# Add is holiday categorical (Saturday and Sunday)\n",
    "train_raw['holiday'] = 0\n",
    "train_raw.loc[(train_raw['measurement_time'].dt.dayofweek == 5) | (train_raw['measurement_time'].dt.dayofweek == 6), 'holiday'] = 1\n",
    "\n",
    "test_raw['holiday'] = 0\n",
    "test_raw.loc[(test_raw['measurement_time'].dt.dayofweek == 5) | (test_raw['measurement_time'].dt.dayofweek == 6), 'holiday'] = 1\n",
    "\n",
    "\n",
    "# Add hour column\n",
    "train_raw['hour'] = train_raw['measurement_time'].dt.hour\n",
    "test_raw['hour'] = test_raw['measurement_time'].dt.hour\n",
    "\n",
    "# Encode 'hour' using splines with 24 knots (one for each hour)\n",
    "train_raw = make_cyclic_features(train_raw, 'hour', 24)\n",
    "test_raw = make_cyclic_features(test_raw, 'hour', 24)\n",
    "\n",
    "# Drop measurement_time\n",
    "train_raw = train_raw.drop(columns=['measurement_time'])\n",
    "test_raw = test_raw.drop(columns=['measurement_time'])\n",
    "\n",
    "# Combine train and test data for imputation\n",
    "combined = pd.concat([train_raw, test_raw], ignore_index=True)\n",
    "\n",
    "# Initialize the IterativeImputer\n",
    "imputer = IterativeImputer(random_state=42)\n",
    "\n",
    "# Fit and transform the combined data\n",
    "imputed_data = imputer.fit_transform(combined)\n",
    "\n",
    "# Convert the imputed data back to DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=combined.columns)\n",
    "\n",
    "# Standardize the data except target, scaling harms performance?\n",
    "# scaler = sk.preprocessing.StandardScaler()\n",
    "# float_columns = imputed_df.select_dtypes(include=['float64']).columns\n",
    "# float_columns = float_columns.drop('target')\n",
    "# float_columns = float_columns.drop('ID')\n",
    "# scaler.fit(imputed_df[float_columns])\n",
    "\n",
    "# scaled_data = scaler.transform(imputed_df[float_columns])\n",
    "# imputed_df[float_columns] = scaled_data\n",
    "\n",
    "# Split the combined data back into train and test\n",
    "train_imputed = imputed_df.iloc[:len(train_raw)].reset_index(drop=True)\n",
    "test_imputed = imputed_df.iloc[len(train_raw):].reset_index(drop=True)\n",
    "\n",
    "# Make IDs int\n",
    "train_imputed['ID'] = train_imputed['ID'].astype(int)\n",
    "test_imputed['ID'] = test_imputed['ID'].astype(int)\n",
    "\n",
    "# Make work_hours and holiday categorical\n",
    "train_imputed['work_hours'] = train_imputed['work_hours'].astype(int)\n",
    "train_imputed['work_hours'] = train_imputed['work_hours'].astype('category')\n",
    "train_imputed['holiday'] = train_imputed['holiday'].astype(int)\n",
    "train_imputed['holiday'] = train_imputed['holiday'].astype('category')\n",
    "\n",
    "# Make work_hours and holiday categorical for test\n",
    "test_imputed['work_hours'] = test_imputed['work_hours'].astype(int)\n",
    "test_imputed['work_hours'] = test_imputed['work_hours'].astype('category')\n",
    "test_imputed['holiday'] = test_imputed['holiday'].astype(int)\n",
    "test_imputed['holiday'] = test_imputed['holiday'].astype('category')\n",
    "\n",
    "# remove target from test\n",
    "test_imputed = test_imputed.drop(columns=['target'])\n",
    "\n",
    "# drop wind_speed, wind_direction, clouds\n",
    "train_imputed = train_imputed.drop(columns=['wind_speed', 'wind_direction', 'clouds'])\n",
    "test_imputed = test_imputed.drop(columns=['wind_speed', 'wind_direction', 'clouds'])\n",
    "\n",
    "# Just drop the radiation columns\n",
    "radiation_columns = ['sun_radiation_east', 'sun_radiation_west', 'sun_radiation_south', 'sun_radiation_north', 'sun_radiation_perpendicular']\n",
    "train_imputed = train_imputed.drop(columns=radiation_columns)\n",
    "test_imputed = test_imputed.drop(columns=radiation_columns)\n",
    "\n",
    "# Save the data\n",
    "train_imputed.to_pickle('data/pure/train.pkl')\n",
    "test_imputed.to_pickle('data/pure/test.pkl')\n",
    "\n",
    "# Add csvs also\n",
    "train_imputed.to_csv('data/pure/train.csv', index=False)\n",
    "test_imputed.to_csv('data/pure/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                          int64\n",
      "target                    float64\n",
      "source_1_temperature      float64\n",
      "source_2_temperature      float64\n",
      "source_3_temperature      float64\n",
      "source_4_temperature      float64\n",
      "mean_room_temperature     float64\n",
      "outside_temperature       float64\n",
      "work_hours               category\n",
      "holiday                  category\n",
      "hour_sin                  float64\n",
      "hour_cos                  float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print data types\n",
    "print(train_imputed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train after removing constant temperature readings: 6958\n",
      "Saving the file in data/constant_removed/train\n",
      "Saving the file in data/constant_removed/test\n"
     ]
    }
   ],
   "source": [
    "def remove_constant_temperature_readings(train_df, periods=4):\n",
    "    \"\"\"\n",
    "    Remove data points where temperature readings remain constant across consecutive periods.\n",
    "\n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training DataFrame.\n",
    "    periods (int): Number of consecutive periods to check for constant temperatures. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame with constant temperature periods removed.\n",
    "    \"\"\"\n",
    "    temperature_columns = [col for col in train_df.columns if 'source_' in col and 'temperature' in col]\n",
    "    mask = pd.Series(True, index=train_df.index)\n",
    "\n",
    "    for col in temperature_columns:\n",
    "        # Calculate differences between consecutive readings\n",
    "        diffs = train_df[col].diff()\n",
    "        \n",
    "        # Create a rolling window and check if all differences are 0\n",
    "        is_constant = diffs.rolling(window=periods).apply(lambda x: (x == 0).all())\n",
    "        \n",
    "        # Update mask to keep only rows where temperatures are changing\n",
    "        mask = mask & (is_constant == False)\n",
    "\n",
    "    return train_df[mask]\n",
    "\n",
    "const_imputed = remove_constant_temperature_readings(train_imputed)\n",
    "\n",
    "# Print length of train\n",
    "print(f'Length of train after removing constant temperature readings: {len(const_imputed)}')\n",
    "\n",
    "# Save the data\n",
    "force_save_data(const_imputed, 'data/constant_removed/train')\n",
    "force_save_data(test_imputed, 'data/constant_removed/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train after removing constant slope periods: 6824\n",
      "Saving the file in data/slope_removed/train\n",
      "Saving the file in data/slope_removed/test\n"
     ]
    }
   ],
   "source": [
    "def remove_constant_slopes(train_df, window=5, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Remove data points where temperature changes are too linear (constant slope).\n",
    "\n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): Training DataFrame\n",
    "    window (int): Size of rolling window to check for linear changes\n",
    "    threshold (float): Maximum allowed variation in second differences to consider slope constant\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame with constant slope periods removed\n",
    "    \"\"\"\n",
    "    temperature_columns = [col for col in train_df.columns if 'source_' in col and 'temperature' in col]\n",
    "    mask = pd.Series(True, index=train_df.index)\n",
    "\n",
    "    for col in temperature_columns:\n",
    "        # Calculate first differences (rate of change)\n",
    "        first_diff = train_df[col].diff()\n",
    "        \n",
    "        # Calculate second differences (acceleration)\n",
    "        second_diff = first_diff.diff()\n",
    "        \n",
    "        # Create rolling window to check if second differences are near zero\n",
    "        is_linear = second_diff.rolling(window=window).apply(\n",
    "            lambda x: abs(x).mean() < threshold\n",
    "        )\n",
    "        \n",
    "        # Update mask to keep only rows where temperature changes are not too linear\n",
    "        mask = mask & (is_linear == False)\n",
    "\n",
    "    return train_df[mask]\n",
    "\n",
    "slope_imputed = remove_constant_slopes(const_imputed)\n",
    "\n",
    "# Print length of train\n",
    "print(f'Length of train after removing constant slope periods: {len(slope_imputed)}')\n",
    "\n",
    "# Save the data\n",
    "force_save_data(slope_imputed, 'data/slope_removed/train')\n",
    "force_save_data(test_imputed, 'data/slope_removed/test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "energy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
