{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main notebook for training with autogluon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['RAY_IGNORE_UNHANDLED_ERRORS'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/Users/akseljoonas/Documents/mlfortnight/data/raw/train_fixed.csv\"\n",
    "sub_path = \"/Users/akseljoonas/Documents/mlfortnight/data/raw/test_fixed.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_path)\n",
    "sub_data = pd.read_csv(sub_path)\n",
    "\n",
    "identifier = \"cbrt_hourcos_isweekend_pruned_3ma60_inter_-13_tempcos-cor\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOUR COS\n",
    "\n",
    "\n",
    "\n",
    "train_data[\"hour_of_day\"] = pd.to_datetime(train_data[\"measurement_time\"]).dt.hour\n",
    "sub_data[\"hour_of_day\"] = pd.to_datetime(sub_data[\"measurement_time\"]).dt.hour\n",
    "\n",
    "train_data[\"hour_of_day_cos\"] = np.cos(2 * np.pi * train_data[\"hour_of_day\"] / 24)\n",
    "sub_data[\"hour_of_day_cos\"] = np.cos(2 * np.pi * sub_data[\"hour_of_day\"] / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS_WEEKEND\n",
    "\n",
    "train_data[\"is_weekend\"] = (\n",
    "    pd.to_datetime(train_data[\"measurement_time\"]).dt.weekday >= 5\n",
    ")\n",
    "sub_data[\"is_weekend\"] = pd.to_datetime(sub_data[\"measurement_time\"]).dt.weekday >= 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVING AVERAGES\n",
    "\n",
    "columns_to_average = [\n",
    "    \"source_1_temperature\",\n",
    "    \"mean_room_temperature\",\n",
    "    \"outside_temperature\",\n",
    "]\n",
    "\n",
    "# Create moving averages for each column in the list\n",
    "for column in columns_to_average:\n",
    "    train_data[f\"{column}_moving_avg\"] = (\n",
    "        train_data[column].rolling(window=12, min_periods=1).mean()\n",
    "    )\n",
    "    sub_data[f\"{column}_moving_avg\"] = (\n",
    "        sub_data[column].rolling(window=12, min_periods=1).mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"source_3_temperature_cos\"] = np.cos(2 * np.pi * train_data[\"source_3_temperature\"] / 24)\n",
    "sub_data[\"source_3_temperature_cos\"] = np.cos(2 * np.pi * sub_data[\"source_3_temperature\"] / 24)\n",
    "\n",
    "train_data[\"source_2_temperature_cos\"] = np.cos(2 * np.pi * train_data[\"source_2_temperature\"] / 24)\n",
    "sub_data[\"source_2_temperature_cos\"] = np.cos(2 * np.pi * sub_data[\"source_2_temperature\"] / 24)\n",
    "\n",
    "train_data[\"source_4_temperature_cos\"] = np.cos(2 * np.pi * train_data[\"source_4_temperature\"] / 24)\n",
    "sub_data[\"source_4_temperature_cos\"] = np.cos(2 * np.pi * sub_data[\"source_4_temperature\"] / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUNING\n",
    "\n",
    "base_drop = [\"hour_of_day\", \"ID\", \"measurement_time\"]\n",
    "prune_drop = [\n",
    "    \"clouds\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\",\n",
    "    \"sun_radiation_perpendicular\",\n",
    "]\n",
    "\n",
    "columns_to_drop = base_drop + prune_drop\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "sub_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERACTION FEATURES\n",
    "\n",
    "\n",
    "\n",
    "def add_interaction_features(\n",
    "    data: pd.DataFrame, submission_data: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Adds interaction features to the datasets.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The main dataset.\n",
    "        submission_data (pd.DataFrame): The submission dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: The datasets with added interaction features.\n",
    "    \"\"\"\n",
    "    # List of selected features for interaction\n",
    "    selected_features = [\n",
    "        \"mean_room_temperature\",\n",
    "        \"outside_temperature\",\n",
    "        \"source_1_temperature\",\n",
    "        \"source_2_temperature\",\n",
    "        \"source_3_temperature\",\n",
    "        \"source_4_temperature\",\n",
    "        \"hour_of_day_cos\",\n",
    "        \"source_1_temperature_moving_avg\",\n",
    "        \"mean_room_temperature_moving_avg\",\n",
    "        \"outside_temperature_moving_avg\",\n",
    "    ]\n",
    "    # Initialize the PolynomialFeatures transformer\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "    # Transform the main dataset\n",
    "    X_selected_data = data[selected_features]\n",
    "    X_interactions_data = poly.fit_transform(X_selected_data)\n",
    "\n",
    "    interaction_feature_names = poly.get_feature_names_out(selected_features)\n",
    "\n",
    "\n",
    "    X_interactions_df_data = pd.DataFrame(\n",
    "        X_interactions_data, columns=interaction_feature_names\n",
    "    )\n",
    "    X_interactions_df_data = X_interactions_df_data.drop(columns=selected_features)\n",
    "    data = pd.concat([data, X_interactions_df_data], axis=1)\n",
    "\n",
    "    # Transform the submission dataset\n",
    "    X_selected_submission = submission_data[selected_features]\n",
    "    X_interactions_submission = poly.transform(X_selected_submission)\n",
    "    X_interactions_df_submission = pd.DataFrame(\n",
    "        X_interactions_submission, columns=interaction_feature_names\n",
    "    )\n",
    "    X_interactions_df_submission = X_interactions_df_submission.drop(\n",
    "        columns=selected_features\n",
    "    )\n",
    "\n",
    "    submission_data = pd.concat([submission_data, X_interactions_df_submission], axis=1)\n",
    "\n",
    "    return data, submission_data\n",
    "\n",
    "\n",
    "train_data, sub_data = add_interaction_features(train_data, sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUBE ROOT TRANSFORMATION OF SELECTED FEATURES\n",
    "\n",
    "include_columns = [\n",
    "    \"source_1_temperature\",\n",
    "    \"source_2_temperature\",\n",
    "    \"source_3_temperature\",\n",
    "    \"mean_room_temperature\",\n",
    "    \"source_4_temperature\",\n",
    "    \"sun_radiation_west\",\n",
    "    \"mean_room_temperature source_1_temperature\",\n",
    "    \"source_2_temperature source_4_temperature\",\n",
    "    \"source_3_temperature source_4_temperature\",\n",
    "    \"source_2_temperature source_3_temperature\",\n",
    "    \"mean_room_temperature source_3_temperature\",\n",
    "]\n",
    "\n",
    "# Apply cube root transformation and rename columns\n",
    "train_data = train_data.apply(\n",
    "    lambda x: x ** (1 / 3) if x.name in include_columns else x\n",
    ").rename(columns={col: f\"cbrt_{col}\" for col in include_columns})\n",
    "\n",
    "sub_data = sub_data.apply(\n",
    "    lambda x: x ** (1 / 3) if x.name in include_columns else x\n",
    ").rename(columns={col: f\"cbrt_{col}\" for col in include_columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL PRUNING BASED ON CORRELATION MATRIX\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"outside_temperature_moving_avg\",\n",
    "    \"mean_room_temperature outside_temperature_moving_avg\",\n",
    "    \"mean_room_temperature_moving_avg outside_temperature_moving_avg\",\n",
    "    \"mean_room_temperature outside_temperature\",\n",
    "    \"outside_temperature\",\n",
    "    \"source_2_temperature outside_temperature_moving_avg\",\n",
    "    \"outside_temperature mean_room_temperature_moving_avg\",\n",
    "    \"hour_of_day_cos outside_temperature_moving_avg\",\n",
    "    \"source_4_temperature outside_temperature_moving_avg\",\n",
    "    \"source_3_temperature outside_temperature_moving_avg\",\n",
    "    \"hour_of_day_cos mean_room_temperature_moving_avg\",\n",
    "    \"outside_temperature outside_temperature_moving_avg\",\n",
    "    \"source_2_temperature source_1_temperature_moving_avg\",\n",
    "    \"source_3_temperature mean_room_temperature_moving_avg\",\n",
    "    \"mean_room_temperature source_1_temperature_moving_avg\",\n",
    "    \"hour_of_day_cos\",\n",
    "    \"source_2_temperature hour_of_day_cos\",\n",
    "    \"hour_of_day_cos source_1_temperature_moving_avg\",\n",
    "    \"mean_room_temperature hour_of_day_cos\",\n",
    "    \"outside_temperature source_3_temperature\",\n",
    "]\n",
    "\n",
    "\n",
    "train_data.drop(columns=columns_to_drop, inplace=True)\n",
    "sub_data.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE CORRELATION MATRIX\n",
    "correlation_matrix = train_data.corr()\n",
    "\n",
    "# Flatten the correlation matrix and sort by absolute value, excluding self-correlations\n",
    "sorted_correlations = (\n",
    "    correlation_matrix.where(~np.eye(correlation_matrix.shape[0], dtype=bool))\n",
    "    .stack()\n",
    "    .abs()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Display the sorted correlation pairs\n",
    "print(\"Cross-correlation matrix (sorted):\")\n",
    "print(sorted_correlations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE THE DATA\n",
    "train_data.to_csv(f\"{identifier}.csv\", index=False)\n",
    "sub_data.to_csv(f\"{identifier}_submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"target\"\n",
    "problem_type = \"regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=label,\n",
    "    problem_type=problem_type,\n",
    "    eval_metric=\"mean_absolute_error\",\n",
    "    path=f\"./AutogluonModels/{identifier}_{train_path.split('/')[-1].split('.')[0]}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = predictor.fit(\n",
    "    # presets=\"best\",\n",
    "    train_data=train_data,\n",
    "    auto_stack=True,\n",
    "    time_limit=600,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(predictor.predict(sub_data))\n",
    "preds[\"ID\"] = pd.read_csv(\"/Users/akseljoonas/Documents/mlfortnight/data/raw/test.csv\")[\n",
    "    \"ID\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_csv(\n",
    "    f\"lol_{identifier}_{train_path.split('/')[-1]}\",\n",
    "    columns=[\"ID\", \"target\"],\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance and leaderboard of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = predictor.feature_importance(train_data)\n",
    "feature_importance.to_csv(f\"importance_{identifier}_{train_path.split('/')[-1]}\")\n",
    "feature_importance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlfortnight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
